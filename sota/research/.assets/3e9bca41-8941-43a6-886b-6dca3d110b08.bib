@article{SEIDENSCHNUR2022104500,
title = {A common data environment for HVAC design and engineering},
journal = {Automation in Construction},
volume = {142},
pages = {104500},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104500},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522003739},
author = {Mikki Seidenschnur and Ali Kücükavci and Esben Visby Fjerbæk and Kevin Michael Smith and Pieter Pauwels and Christian Anker Hviid},
keywords = {Building information modeling, HVAC, Object models, Common data environment, BIM level 3},
abstract = {The Architecture, Engineering, and Construction (AEC) industry is transitioning toward using cloud-based Common Data Environments (CDEs) with interlinked BIM models. A CDE that engages all stakeholders of the building's design, construction, and operation phases represents the outset of BIM maturity level 3. This article introduces a CDE called Virtual Commissioning (VC), capable of commissioning an HVAC system before the physical commissioning of the HVAC system. The FSC diagram is introduced, to represent an HVAC BIM model within the VC CDE, and the Revit to FSC exporter, to serialize an HVAC object model from Revit to the FSC diagram. Three microservices were developed to exemplify the ease of developing independently scalable solutions for the VC CDE. Furthermore, the article proves that Modelica simulations can be run, using the microservice architecture of the CDE. To test the robustness of the system architecture for the CDE, two example models were introduced, one simple and one with a high level of complexity. Transferring the example models from Revit to the VC CDE was successful. Finally, in the roadmap for future development, it is proposed that future work should focus on using the CDE for advanced hydraulic simulations, using Modelica and Spawn-of-EnergyPlus.}
}
@incollection{RAJ2018279,
title = {Chapter Seven - The Hadoop Ecosystem Technologies and Tools},
editor = {Pethuru Raj and Ganesh Chandra Deka},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {109},
pages = {279-320},
year = {2018},
booktitle = {A Deep Dive into NoSQL Databases: The Use Cases and Applications},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2017.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0065245817300475},
author = {Pethuru Raj},
keywords = {Big data analytics (BDA), Hybrid clouds, Hadoop, Knowledge discovery and dissemination, Decision enablement, The Internet of Things (IoT), Apache Spark (batch processing) and Spark Streaming, HDFS storage, Kafka message queue/broker, NoSQL databases},
abstract = {There are several interesting and inspiring trends and transitions succulently happening in the business as well as IT spaces. One noteworthy factor and fact is that there are fresh data sources emerging and pouring out a lot of usable and reusable data. With the number of different, distributed, and decentralized data sources is consistently on the rise, the resulting data scope, size, structure, schema, and speed are greatly changing and challenging too. The other dominant and prominent aspects include polyglot microservices are solidifying deeply as the new building and deployment/execution block in the software world toward the much-needed accelerated software design, development, deployment, and delivery. The device ecosystem expands frenetically with the arrival of trendy and handy, slim and sleek, disappearing and disposable gadgets, gizmos thereby ubiquitous (anywhere, anytime, and any device) access, and usage of web-scale information, content, and services get fructified. Finally, all sorts of casually found and cheap articles in our everyday environments (homes, hotels, hospitals, etc.) are being systematically digitized and service enabled in order to exhibit a kind of real-world smartness and sagacity in their individual as well as collective actions and reactions. Thus trillions of digitized objects, billions of connected devices, and millions of polyglot software services are bound to interact insightfully with one another over locally as well as with remote ones over any networks purposefully. And hence the amount of transactional, operational, analytical, commercial, social, personal, and professional data created through a growing array of interactions and collaborations is growing very rapidly. Now if the data getting collected, processed, and stocked are not subjected to deeper, deft, and decisive investigations, then the tactically as well as strategically sound knowledge (the beneficial patterns, tips, techniques, associations, alerts, risk factors, fresh opportunities, possibilities, etc.) hidden inside the data heaps goes unused literally. For collecting, stocking, and processing such a large amount of multistructured data, the traditional databases, analytics platforms, the ETL tools, etc., are found insufficient. Hence the Apache Hadoop ecosystem technologies and tools are being touted as the best way forward to squeeze out the right and relevant knowledge. In this chapter, you can find the details about the emerging technologies and platforms for spearheading the big data movement.}
}
@article{JOSEPH2020101785,
title = {IntMA: Dynamic Interaction-aware resource allocation for containerized microservices in cloud environments},
journal = {Journal of Systems Architecture},
volume = {111},
pages = {101785},
year = {2020},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2020.101785},
url = {https://www.sciencedirect.com/science/article/pii/S1383762120300758},
author = {Christina Terese Joseph and K. Chandrasekaran},
keywords = {Microservice placement, Container orchestration, Service oriented computing, Cloud computing, Performance optimization},
abstract = {The Information Technology sector has undergone tremendous changes arising due to the emergence and prevalence of Cloud Computing. Microservice Architectures have also been attracting attention from several industries and researchers. Due to the suitability of microservices for the Cloud environments, an increasing number of Cloud applications are now provided as microservices. However, this transition to microservices brings a wide range of infrastructural orchestration challenges. Though several research works have discussed the engineering of microservice-based applications, there is an inevitable need for research on handling the operational phases of the microservice components. Microservice application deployment in containerized datacenters must be optimized to enhance the overall system performance. In this research work, the deployment of microservice application modules on the Cloud infrastructure is first modelled as a Binary Quadratic Programming Problem. In order to reduce the adverse impact of communication latencies on the response time, the interaction pattern between the microservice components is modelled as an undirected doubly weighted complete Interaction Graph. A novel, robust heuristic approach IntMA is also proposed for deploying the microservices in an interaction-aware manner with the aid of the interaction information obtained from the Interaction Graph. The proposed allocation policies are implemented in Kubernetes. The effectiveness of the proposed approach is evaluated on the Google Cloud Platform, using different microservice reference applications. Experimental results indicate that the proposed approach improves the response time and throughput of the microservice-based systems.}
}
@article{MOGHADDAM2019194,
title = {Design of Marketplaces for Smart Manufacturing Services},
journal = {Procedia Manufacturing},
volume = {39},
pages = {194-201},
year = {2019},
note = {25th International Conference on Production Research Manufacturing Innovation: Cyber Physical Manufacturing August 9-14, 2019 | Chicago, Illinois (USA)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.312},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920303796},
author = {Mohsen Moghaddam and Albert Jones and Thorsten Wuest},
keywords = {Service-oriented manufacturing, Cyber-physical systems, Matching markets, Multi-agent reinforcement learning, Industry 4.0},
abstract = {The future of manufacturing depends on the successful implementation of two terms: cyber-physical systems (CPS) and micro-services. CPS technologies are transforming the way manufacturing components interact, just as the Internet has transformed the interaction with information. A micro-service is an architectural style that structures an application as a collection of loosely-coupled, independent, and self-executable programs that can be composed to provide various capabilities of large, monolithic manufacturing applications such as ERP and MES. The transition to such a modular structure enables two important features. First, micro-services can be sourced on a pay-as-you-go fashion from a large, diverse, and growing number of independent providers, thus opening the manufacturing space to new players such as innovative and agile startups. Second, those micro-services can be viewed as building-blocks for creating on-demand, complex, custom processes that address the specific needs of users. Taken together, micro-services, the platforms on which they reside, and service vendors constitute a multi-platform marketplace. This article first provides an overview of new manufacturing paradigms enabled by CPS and micro-services. It then formalizes the multi-platform marketplaces for micro-services as multi-layer networks of collaborating-competing agents with different interests, policies, capabilities, and technologies. It finally formulates a research methodology for optimizing the policies and interactions through the theories of matching markets and multi-agent reinforcement learning.}
}
@article{SCHERMANN201841,
title = {We’re doing it live: A multi-method empirical study on continuous experimentation},
journal = {Information and Software Technology},
volume = {99},
pages = {41-57},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917302136},
author = {Gerald Schermann and Jürgen Cito and Philipp Leitner and Uwe Zdun and Harald C. Gall},
keywords = {Release engineering, Continuous deployment, Continuous experimentation, Empirical study},
abstract = {Context
Continuous experimentation guides development activities based on data collected on a subset of online users on a new experimental version of the software. It includes practices such as canary releases, gradual rollouts, dark launches, or A/B testing.
Objective
Unfortunately, our knowledge of continuous experimentation is currently primarily based on well-known and outspoken industrial leaders. To assess the actual state of practice in continuous experimentation, we conducted a mixed-method empirical study.
Method
In our empirical study consisting of four steps, we interviewed 31 developers or release engineers, and performed a survey that attracted 187 complete responses. We analyzed the resulting data using statistical analysis and open coding.
Results
Our results lead to several conclusions: (1) from a software architecture perspective, continuous experimentation is especially enabled by architectures that foster independently deployable services, such as microservices-based architectures; (2) from a developer perspective, experiments require extensive monitoring and analytics to discover runtime problems, consequently leading to developer on call policies and influencing the role and skill sets required by developers; and (3) from a process perspective, many organizations conduct experiments based on intuition rather than clear guidelines and robust statistics.
Conclusion
Our findings show that more principled and structured approaches for release decision making are needed, striving for highly automated, systematic, and data- and hypothesis-driven deployment and experimentation.}
}
@article{DIFRANCESCO201977,
title = {Architecting with microservices: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {150},
pages = {77-97},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300019},
author = {Paolo {Di Francesco} and Patricia Lago and Ivano Malavolta},
keywords = {Microservices, Software architecture, Systematic mapping study},
abstract = {Context
A microservice architecture is composed of a set of small services, each running in its own process and communicating with lightweight mechanisms. Many aspects on architecting with microservices are still unexplored and existing research is still far from being crispy clear.
Objective
We aim at identifying, classifying, and evaluating the state of the art on architecting with microservices from the following perspectives: publication trends, focus of research, and potential for industrial adoption.
Method
We apply the systematic mapping methodology. We rigorously selected 103 primary studies and we defined and applied a classification framework to them for extracting key information for subsequent analysis. We synthesized the obtained data and produced a clear overview of the state of the art.
Results
This work contributes with (i) a classification framework for research studies on architecting with microservices, (ii) a systematic map of current research of the field, (iii) an evaluation of the potential for industrial adoption of research results, and (iv) a discussion of emerging findings and implications for future research.
Conclusion
This study provides a solid, rigorous, and replicable picture of the state of the art on architecting with microservices. Its results can benefit both researchers and practitioners of the field.}
}
@article{LENARDUZZI2020110710,
title = {Does migrating a monolithic system to microservices decrease the technical debt?},
journal = {Journal of Systems and Software},
volume = {169},
pages = {110710},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110710},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220301539},
author = {Valentina Lenarduzzi and Francesco Lomio and Nyyti Saarimäki and Davide Taibi},
keywords = {Technical debt, Architectural debt, Code quality, Microservices, Refactoring},
abstract = {Background:
The migration from a monolithic system to microservices requires a deep refactoring of the system. Therefore, such a migration usually has a big economic impact and companies tend to postpone several activities during this process, mainly to speed up the migration itself, but also because of the demand for releasing new features.
Objective:
We monitored the technical debt of an SME while it migrated from a legacy monolithic system to an ecosystem of microservices. Our goal was to analyze changes in the code technical debt before and after the migration to microservices.
Method:
We conducted a case study analyzing more than four years of the history of a twelve-year-old project (280K Lines of Code) where two teams extracted five business processes from the monolithic system as microservices. For the study, we first analyzed the technical debt with SonarQube and then performed a qualitative study with company members to understand the perceived quality of the system and the motivation for possibly postponed activities.
Results:
The migration to microservices helped to reduce the technical debt in the long run. Despite an initial spike in the technical debt due to the development of the new microservice, after a relatively short period of time the technical debt tended to grow slower than in the monolithic system.}
}
@article{SIMPKIN201970,
title = {Constructing distributed time-critical applications using cognitive enabled services},
journal = {Future Generation Computer Systems},
volume = {100},
pages = {70-85},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18311762},
author = {Chris Simpkin and Ian Taylor and Graham A. Bent and Geeth {de Mel} and Swati Rallapalli and Liang Ma and Mudhakar Srivatsa},
keywords = {Decentralized workflows, Quality of service, Quality of experience, Vector symbolic architectures, Distributed fitness functions, Time-critical applications, Dynamic wireless networks},
abstract = {Time-critical analytics applications are increasingly making use of distributed service interfaces (e.g., micro-services) that support the rapid construction of new applications by dynamically linking the services into different workflow configurations. Traditional service-based applications, in fixed networks, are typically constructed and managed centrally and assume stable service endpoints and adequate network connectivity. Constructing and maintaining such applications in dynamic heterogeneous wireless networked environments, where limited bandwidth and transient connectivity are commonplace, presents significant challenges and makes centralized application construction and management impossible. In this paper we present an architecture which is capable of providing an adaptable and resilient method for on-demand decentralized construction and management of complex time-critical applications in such environments. The approach uses a Vector Symbolic Architecture (VSA) to compactly represent an application as a single semantic vector that encodes the service interfaces, workflow, and the time-critical constraints required. By extending existing services interfaces, with a simple cognitive layer that can interpret and exchange the vectors, we show how the required services can be dynamically discovered and interconnected in a completely decentralized manner. We demonstrate the viability of this approach by using a VSA to encode various time-critical data analytics workflows. We show that these vectors can be used to dynamically construct and run applications using services that are distributed across an emulated Mobile Ad-Hoc Wireless Network (MANET). Scalability is demonstrated via an empirical evaluation.}
}
@article{SILVA2021301197,
title = {Using micro-services and artificial intelligence to analyze images in criminal evidences},
journal = {Forensic Science International: Digital Investigation},
volume = {37},
pages = {301197},
year = {2021},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2021.301197},
url = {https://www.sciencedirect.com/science/article/pii/S2666281721001050},
author = {Iaslan Silva and João Marcos {do Valle} and Gabriel Souza and Jaine Budke and Daniel Araújo and Bruno Carvalho and Nélio Cacho and Henrique Sales and Frederico Lopes and Rivaldo {Silva Júnior}},
keywords = {Computer vision, Digital forensics, Crime evidence, Machine learning, Microservice},
abstract = {With the advancement of digital crimes, the field of digital forensic science grows more and more, and with this growth, the search for faster and more accurate solutions to aid the investigation process becomes a necessity. In the context of the Brazilian judicial system, during a criminal investigation, forensic specialists extract, decode, and analyze the evidence collected to allow the prosecutor to make legal demands for a prosecution. These specialists have a very short time to analyze to find criminal evidence and the process can take a long time. To solve this problem this paper proposes to use a micro-services-based application with artificial intelligence to process large amounts of images contained in criminal evidence using open-source software. The image classification module contains some pre-trained classifiers, considering the needs of forensic analysts of the Rio Grande do Norte District Attorney's Office (MPRN). The models were built to identify specific types of objects, for example, firearms, ammunition, Brazilian identity cards, text documents, cell phone screen captures, and nudity. The results obtained show that the system achieved good accuracy in most cases. This is extremely important in the context of this research, where false positives should be avoided in order to save analysts' working time. Moreover, the proposed architecture was able to speed up the image classification process using Apache Spark.}
}
@article{MARTIN20191,
title = {Mapping heterogeneous research infrastructure metadata into a unified catalogue for use in a generic virtual research environment},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {1-13},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.05.076},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19302699},
author = {Paul Martin and Laurent Remy and Maria Theodoridou and Keith Jeffery and Zhiming Zhao},
keywords = {Virtual research environment, Science gateway, Research infrastructure, Metadata catalogue, Metadata mapping},
abstract = {Virtual Research Environments (VREs), also known as science gateways or virtual laboratories, assist researchers in data science by integrating tools for data discovery, data retrieval, workflow management and researcher collaboration, often coupled with a specific computing infrastructure. Recently, the push for better open data science has led to the creation of a variety of dedicated research infrastructures (RIs) that gather data and provide services to different research communities, all of which can be used independently of any specific VRE. There is therefore a need for generic VREs that can be coupled with the resources of many different RIs simultaneously, easily customised to the needs of specific communities. The resource metadata produced by these RIs rarely all adhere to any one standard or vocabulary however, making it difficult to search and discover resources independently of their providers without some translation into a common framework. Cross-RI search can be expedited by using mapping services that harvest RI-published metadata to build unified resource catalogues, but the development and operation of such services pose a number of challenges. In this paper, we discuss some of these challenges and look specifically at the VRE4EIC Metadata Portal, which uses X3ML mappings to build a single catalogue for describing data products and other resources provided by multiple RIs. The Metadata Portal was built in accordance to the e-VRE Reference Architecture, a microservice-based architecture for generic modular VREs, and uses the CERIF standard to structure its catalogued metadata. We consider the extent to which it addresses the challenges of cross-RI search, particularly in the environmental and earth science domain, and how it can be further augmented, for example to take advantage of linked vocabularies to provide more intelligent semantic search across multiple domains of discourse.}
}
@article{ABDULLAH2019243,
title = {Unsupervised learning approach for web application auto-decomposition into microservices},
journal = {Journal of Systems and Software},
volume = {151},
pages = {243-257},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.02.031},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300408},
author = {Muhammad Abdullah and Waheed Iqbal and Abdelkarim Erradi},
keywords = {Application decomposition, Scalability, Microservices, Web applications, Cloud computing},
abstract = {Nowadays, large monolithic web applications are manually decomposed into microservices for many reasons including adopting a modern architecture to ease maintenance and increase reusability. However, the existing approaches to refactor a monolithic application do not inherently consider the application scalability and performance. We devise a novel method to automatically decompose a monolithic application into microservices to improve the application scalability and performance. Our proposed decomposition method is based on a black-box approach that uses the application access logs and an unsupervised machine-learning method to auto-decompose the application into microservices mapped to URL partitions having similar performance and resource requirements. In particular, we propose a complete automated system to decompose an application into microservices, deploy the microservices using appropriate resources, and auto-scale the microservices to maintain the desired response time. We evaluate the proposed system using real web applications on a public cloud infrastructure. The experimental evaluation shows an improved performance of the auto-created microservices compared with the monolithic version of the application and the manually created microservices.}
}
@article{HANNOUSSE2021100415,
title = {Securing microservices and microservice architectures: A systematic mapping study},
journal = {Computer Science Review},
volume = {41},
pages = {100415},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100415},
url = {https://www.sciencedirect.com/science/article/pii/S1574013721000551},
author = {Abdelhakim Hannousse and Salima Yahiouche},
keywords = {Microservices, Microservice architectures, Security, Systematic mapping},
abstract = {Microservice architectures (MSA) are becoming trending alternatives to existing software development paradigms notably for developing complex and distributed applications. Microservices emerged as an architectural design pattern aiming to address the scalability and ease the maintenance of online services. However, security breaches have increased threatening availability, integrity and confidentiality of microservice-based systems. A growing body of literature is found addressing security threats and security mechanisms to individual microservices and microservice architectures. The aim of this study is to provide a helpful guide to developers about already recognized threats on microservices and how they can be detected, mitigated or prevented; we also aim to identify potential research gaps on securing MSA. In this paper, we conduct a systematic mapping in order to categorize threats on MSA with their security proposals. Therefore, we extracted threats and details of proposed solutions reported in selected studies. Obtained results are used to design a lightweight ontology for security patterns of MSA. The ontology can be queried to identify source of threats, security mechanisms used to prevent each threat, applicability layer and validation techniques used for each mechanism. The systematic search yielded 1067 studies of which 46 are selected as primary studies. The results of the mapping revealed an unbalanced research focus in favor of external attacks; auditing and enforcing access control are the most investigated techniques compared with prevention and mitigation. Additionally, we found that most proposed solutions are soft-infrastructure applicable layer compared with other layers such as communication and deployment. We also found that performance analysis and case studies are the most used validation techniques of security proposals.}
}
@article{SELLAMI2022106996,
title = {Improving microservices extraction using evolutionary search},
journal = {Information and Software Technology},
volume = {151},
pages = {106996},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106996},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001264},
author = {Khaled Sellami and Ali Ouni and Mohamed Aymen Saied and Salah Bouktif and Mohamed Wiem Mkaouer},
keywords = {Microservices, Search-based software engineering, Legacy decomposition, Microservices architecture},
abstract = {Context:
Microservices constitute a modern style of building software applications as collections of small, cohesive, and loosely coupled services, i.e., modules, that are developed, deployed, and scaled independently.
Objective:
The migration from legacy systems towards the microservice-based architecture is not a trivial task. It is still manual, time-consuming, error-prone and subsequently costly. The most critical and challenging issue is the cost-effective identification of microservices boundaries that ensure adequate granularity and cohesiveness.
Method:
To address this problem, we introduce in this paper a novel approach, named MSExtractor , that formulates microservices identification as a multi-objective optimization problem. The proposed solution aims at decomposing a legacy application into a set of cohesive, loosely-coupled and coarse-grained services. We employ the Indicator-Based Evolutionary Algorithm (IBEA) to drive a search process towards optimal microservices identification while considering structural and semantic dependencies in the source code.
Results:
We conduct an empirical evaluation on a benchmark of seven software systems to assess the efficiency of our approach. Results show that MSExtractor is able to carry out an effective identification of relevant microservice candidates and outperforms three other existing approaches.
Conclusion:
In this paper, we show that MSExtractor is able to extract cohesive and loosely coupled services with higher performance than three other considered methods. However, we advocate that while automated microservices identification approaches are very helpful, the role of the human experts remains crucial to validate and calibrate the extracted microservices.}
}
@article{LI2022106992,
title = {Microservice extraction based on knowledge graph from monolithic applications},
journal = {Information and Software Technology},
volume = {150},
pages = {106992},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106992},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001240},
author = {Zhiding Li and Chenqi Shang and Jianjie Wu and Yuan Li},
keywords = {Microservice extraction, Knowledge graph, Monolithic architecture, Constrained Louvain algorithm},
abstract = {Context
Re-architecting monolithic systems with microservice architecture is a common trend. However, determining the "optimal" size of individual services during microservice extraction has been a challenge in software engineering. Common limitations of the literature include not being reasonable enough to be put into practical application; relying too much on human experience; neglection of the impact of hardware environment on the performance.
Objective
To address these problems, this paper proposes a novel method based on knowledge-graph to support the extraction of microservices during the initial phases of re-architecting existing applications.
Method
According to the microservice extraction method based on the AKF principle which is a widely practiced microservice design principle in the industry, four kinds of entities and four types of entity-entity relationships are designed and automatically extracted from specification and design artifacts of the monolithic application to build the knowledge graph. A constrained Louvain algorithm is proposed to identify microservice candidates.
Results
Our approach is tested based on two open-source projects with the other three typical methods: the domain-driven design-based method, the similarity calculation-based method, and the graph clustering-based method . Conducted experiments show that our method performs well concerning all the evaluation metrics.}
}
@article{LI2019110380,
title = {A dataflow-driven approach to identifying microservices from monolithic applications},
journal = {Journal of Systems and Software},
volume = {157},
pages = {110380},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219301475},
author = {Shanshan Li and He Zhang and Zijia Jia and Zheng Li and Cheng Zhang and Jiaqi Li and Qiuya Gao and Jidong Ge and Zhihao Shan},
keywords = {Software engineering, Microservices, Monolith, Decomposition, Data flow, Business logic(s)},
abstract = {Microservices architecture emphasizes employing multiple small-scale and independently deployable microservices, rather than encapsulating all function capabilities into one monolith. Correspondingly, microservice-oriented decomposition, which has been identified to be an extremely challenging task, plays a crucial and prerequisite role in developing microservice-based systems. To address the challenges in such a task, we propose a dataflow-driven semi-automatic decomposition approach. In particular, a four-step decomposition procedure is defined: (1) conduct the business requirement analysis to generate use case and business logic specification; (2) construct the fine-grained Data Flow Diagrams (DFD) and the process-datastore version of DFD (DFDPS) representing the business logics; (3) extract the dependencies between processes and datastores into decomposable sentence sets; and (4) identify candidate microservices by clustering processes and their closely related datastores into individual modules from the decomposable sentence sets. To validate this microservice-oriented decomposition approach, we performed a case study on Cargo Tracking System that is a typical case decomposed by other microservices identification methods (Service Cutter and API Analysis), and made comparisons in terms of specific coupling and cohesion metrics. The results show that the proposed dataflow-driven decomposition approach can recommend microservice candidates with sound coupling and cohesion through a rigorous and easy-to-operate implementation with semi-automatic support.}
}
@article{ZHOU2023111521,
title = {Revisiting the practices and pains of microservice architecture in reality: An industrial inquiry},
journal = {Journal of Systems and Software},
volume = {195},
pages = {111521},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111521},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222001972},
author = {Xin Zhou and Shanshan Li and Lingli Cao and He Zhang and Zijia Jia and Chenxing Zhong and Zhihao Shan and Muhammad Ali Babar},
keywords = {Microservices, Empirical study, Interview, Software architecture},
abstract = {Background:
Seeking an appropriate architecture for the design of software is always a challenge. Although microservices are claimed to be a lightweight architecture style that can improve current practices with several characteristics, many practices are based on different circumstances and reflect variant effects. Empirical inquiry gives us a systematic insight into industrial practices and sufferings on microservices.
Objective:
This study is to investigate the gaps between ideal visions and real industrial practices in microservices and what expenses microservices bring to industrial practitioners.
Method:
We carried out a series of industrial interviews with practitioners from 20 software companies. The collected data were then codified using qualitative methods.
Results:
Eight pairs of common practices and pains of microservices in industry were obtained after synthesizing the rich and detailed data collected. Five aspects that require careful decisions were extracted to help practitioners balance the possible benefits and pains of MSA. Five research directions that need further exploration were identified based on the pains associated with MSA.
Conclusion:
While the benefits of microservices are confirmed from the point of view of practitioners, decisions should be carefully made and the possible problems identified must be addressed with additional expense from experience. Furthermore, some of the topics and pains outlined, e.g., systematic evaluation and assessment, organizational transformation, decomposition, distributed monitoring, and bug localization, may inspire researchers to conduct further research.}
}
@article{BARCLAY2022236,
title = {Trustable service discovery for highly dynamic decentralized workflows},
journal = {Future Generation Computer Systems},
volume = {134},
pages = {236-246},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.03.035},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22001169},
author = {Iain Barclay and Chris Simpkin and Graham Bent and Tom {La Porta} and Declan Millar and Alun Preece and Ian Taylor and Dinesh Verma},
keywords = {Decentralized workflows, Vector Symbolic Architectures, Workflow orchestration, Self-sovereign identity, Trusted services, Dynamic wireless networks},
abstract = {The quantity and capabilities of smart devices and sensors deployed as part of the Internet of Things (IoT) and accessible via remote microservices is set to rise dramatically as the provision of interactive data streaming increases. This introduces opportunities to rapidly construct new applications by interconnecting these microservices in different workflow configurations. The challenge is to discover the required microservices, including those from trusted partners and the wider community, whilst being able to operate robustly under diverse networking conditions. This paper outlines a workflow approach that provides decentralized discovery and orchestration of verifiably trustable services in support of multi-party operations. The approach is based on adoption of patterns from self-sovereign identity research, notably Verifiable Credentials, to share information amongst peers based on attestations of service descriptions and prior service usage in a privacy preserving and secure manner. This provides a dynamic, trust-based framework for ratifying and evaluating the qualities of different services. Collating these new service descriptions and integrating with existing decentralized workflow research based on vector symbolic architecture (VSA) provides an enhanced semantic search space for efficient and trusted service discovery that is necessary to support a diverse range of emerging edge-computing environments. An architecture for a dynamic decentralized service discovery system, is designed, and described through application to a scenario which uses trusted peers’ reported experiences of an anomaly detection service to determine service selection.}
}
@article{MISHRA2022102836,
title = {vServiceInspector: Introspection-assisted evolutionary bag-of-ngram approach to detect malware in cloud servers},
journal = {Ad Hoc Networks},
volume = {131},
pages = {102836},
year = {2022},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2022.102836},
url = {https://www.sciencedirect.com/science/article/pii/S1570870522000439},
author = {Preeti Mishra and Akansh Gupta and Palak Aggarwal and Emmanuel S. Pilli},
keywords = {Malware detection, Cloud security, System call tracing, Genetic algorithm, Convolutional neural networks, Artificial intelligence, Intrusion detection, Evolutionary optimization algorithm},
abstract = {Cloud computing has become very popular and extremely demanding in the market. Several emerging technologies such as Industrial Internet of Things (IIoT), microservices and Bigdata analytics etc. are adopting cloud computing due to the availability of the high-end computing servers. However, security breaches have also started to grow along with its popularity. The advanced malware can target virtualization-based infrastructure and can harm virtual resources and thereby becoming threat to industrial applications & data hosted in cloud. The modern malware are difficult to be detected by using traditional security tools. In this paper, an introspection-assisted evolutionary bag-of-ngram approach is proposed, named as vServiceInspector for doing process monitoring from both inside the virtual machine (In-VM) & outside virtual machine (Out-VM). It employs advanced memory introspection to extract the system call sequences at Out-VM location (i.e. hypervisor). Genetic Algorithm (GA) is employed to find the most discriminating sequences of system calls and extract optimal feature set. Convolutional Neural Network (CNN), a deep learning algorithm is then used to learn and detect the malicious program execution patterns. An accuracy of 83.13%–99.63% is achieved by using University of New Mexico (UNM) dataset and an accuracy of 97.8%–99% is achieved by using University of California (Barecloud) dataset. The vServiceInspector is more accurate and more attack resilient when compared to previously proposed techniques.}
}
@article{ARMANDOBARRONLUGO2021105173,
title = {A novel transversal processing model to build environmental big data services in the cloud},
journal = {Environmental Modelling & Software},
volume = {144},
pages = {105173},
year = {2021},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2021.105173},
url = {https://www.sciencedirect.com/science/article/pii/S1364815221002152},
author = {J. {Armando Barron-Lugo} and Jose Luis Gonzalez-Compean and Jesus Carretero and Ivan Lopez-Arevalo and Raffaele Montella},
keywords = {Big data, Cloud computing, Environmental data, Climate data, Machine learning, Data analytic},
abstract = {This paper presents a novel transversal, agnostic-infrastructure, and generic processing model to build environmental big data services in the cloud. Transversality is used for building processing structures (PS) by reusing/coupling multiple existent software for processing environmental monitoring, climate, and earth observation data, even in execution time, with datasets available in cloud-based repositories. Infrastructure-agnosticism is used for deploying/executing PSs on/in edge, fog, and/or cloud. Genericity is used to embed analytic, merging information, machine learning, and statistic micro-services into PSs for automatically and transparently converting PSs into big data services to support decision-making procedures. A prototype was developed for conducting case studies based on the data climate classification, earth observation products, and making predictions of air data pollution by merging different monitoring climate data sources. The experimental evaluation revealed the efficacy and flexibility of this model to create complex environmental big data services.}
}
@article{LI2021106449,
title = {Understanding and addressing quality attributes of microservices architecture: A Systematic literature review},
journal = {Information and Software Technology},
volume = {131},
pages = {106449},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106449},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301993},
author = {Shanshan Li and He Zhang and Zijia Jia and Chenxing Zhong and Cheng Zhang and Zhihao Shan and Jinfeng Shen and Muhammad Ali Babar},
keywords = {Microservices, Monolith, Quality attributes, Systematic literature review},
abstract = {Context: As a rapidly adopted architectural style in software engineering, Microservices Architecture (MSA) advocates implementing small-scale and independently distributed services, rather than binding all functions into one monolith. Although many initiatives have contributed to the quality improvement of microservices-based systems, there is still a lack of a systematic understanding of the Quality Attributes (QAs) associated with MSA. Objective: This study aims to investigate the evidence-based state-of-the-art of QAs of microservices-based systems. Method: We carried out a Systematic Literature Review (SLR) to identify and synthesize the relevant studies that report evidence related to QAs of MSA. Results: Based on the data extracted from the 72 selected primary studies, we portray an overview of the six identified QAs most concerned in MSA, scalability, performance, availability, monitorability, security, and testability. We identify 19 tactics that architecturally address the critical QAs in MSA, including two tactics for scalability, four for performance, four for availability, four for monitorability, three for security, and two for testability. Conclusion: This SLR concludes that for MSA-based systems: 1) Although scalability is the commonly acknowledged benefit of MSA, it is still an indispensable concern among the identified QAs, especially when trading-off with other QAs, e.g., performance. Apart from the six identified QAs in this study, other QAs for MSA like maintainability need more attention for effective improvement and evaluation in the future. 3) Practitioners need to carefully make the decision of migrating to MSA based on the return on investment, since this architectural style additionally cause some pains in practice.}
}
@article{DEBAUCHE20227494,
title = {Cloud and distributed architectures for data management in agriculture 4.0 : Review and future trends},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {9},
pages = {7494-7514},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821002664},
author = {Olivier Debauche and Saïd Mahmoudi and Pierre Manneback and Frédéric Lebeau},
keywords = {Agriculture 4.0, Smart farming, Smart agriculture, Lambda architecture, Kappa architecture, Edge computing, Fog computing, Micro-service architecture, Data lake, Data house, Blockchain, Osmotic computing, Dew computing},
abstract = {The Agriculture 4.0, also called Smart Agriculture or Smart Farming, is at the origin of the production of a huge amount of data that must be collected, stored, and processed in a very short time. Processing this massive quantity of data needs to use specific infrastructure that use adapted IoT architectures. Our review offers a comparative panorama of Central Cloud, Distributed Cloud Architectures, Collaborative Computing Strategies, and new trends used in the context of Agriculture 4.0. In this review, we try to answer 4 research questions: (1) Which storage and processing architectures are best suited to Agriculture 4.0 applications and respond to its peculiarities? (2) Can generic architectures meet the needs of Agriculture 4.0 application cases? (3) What are the horizontal development possibilities that allow the transition from research to industrialization? (4) What are the vertical valuations possibilities to move from algorithms trained in the cloud to embedded or stand-alone products? For this, we compare architectures with 8 criteria (User Proximity, Latency & Jitter, Network stability, high throughput, Reliability, Scalability, Cost Effectiveness, Maintainability), and analyze the advantages and disadvantages of each of them.}
}
